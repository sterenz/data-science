{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External modules used in this documentation\n",
    "\n",
    "In this short code section we upload all the needed modules for this documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "from pandas import *\n",
    "from matplotlib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software documentation\n",
    "\n",
    "1. [Introduction](#introduction) \n",
    "2. [Data description](#data_description)\n",
    "3. [Software organization](#software_organization)\n",
    "    - [requirements.txt](#requirements.txt)\n",
    "    - [const.py](#const.py)\n",
    "    - [URIs.py](#URIs.py)\n",
    "    - [data_model.py](#data_model.py)\n",
    "    - [common_utils.py](#common_utils.py)\n",
    "    - [main.py](#main.py)\n",
    "    - [data_processors.py](#data_processors.py)\n",
    "    - [relational_processor.py](#relational_processor.py)\n",
    "    - [triplestore_processor.py](#triplestore_processor.py)\n",
    "    - [query_processors.py](#query_processors.py)\n",
    "    - [queries.py](#queries.py)\n",
    "    \n",
    "4. [Functioning](#functioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div id=introduction> </div>\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "### Project goal\n",
    "Our goal is to build a software that enables to populate two kind of databases, a relational and a graph database, and to query these databases simultaneously.  \n",
    "In this project we will deal with *structured data* coming from CSV and JSON files. For processing the data in Python we will use [***pandas***](https://pandas.pydata.org/) , a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive.\n",
    "\n",
    "After shortly analysing the provided data and their characteristics, we developed the processors that will extract the data from the provided CSVs and JSONs, in order to populate our structured collection of data. We will than upload the records created into our databases, as well as providing the possibility to query these databases simultaneously and return specific Python objects.\n",
    "\n",
    "### Design and syntax choices\n",
    "\n",
    "#### Type annotation\n",
    "Since an important characteristic of the Python language is the consistency of its object model. Each object has an associated type (e.g., integer, string, or function) and internal data. We choose, for a better readability, to make the Data Types of the input parameters (arguments) and the output of the function explicit by the use of type annotation.\n",
    "E.g.  \n",
    "\n",
    "    def set_df(self, _df: DataFrame) -> None:\n",
    "\n",
    "This meas that `set_df` will take a DataFrame as input and will return anything, so None. Note, though, that this is syntactic sugar only, Python will not raise an error if the Data Type of the arguments specified in the annotation is not respected (e.g. string, integer, list,…).\n",
    "    \n",
    "#### Try/except statement\n",
    "We made use of the `try/except` statement in the code to handle errors. The `try/except` statement works as follows:\n",
    "\n",
    "- First, the try clause (the statement(s) between the try and except keywords) is executed.\n",
    "- If no exception occurs, the except clause is skipped and execution of the try statement is finished.\n",
    "- If an exception occurs during the execution of the try clause, the rest of the clause is skipped. Then, if its type matches the exception named after the except keyword, the except clause is executed, and then execution continues after the try/except block.\n",
    "\n",
    "Is it also possible to catch the specific Python error in the except statement with the build-in class `Exception` and print it in the terminal.\n",
    "\n",
    "### *set* and *get*\n",
    "We tried to use as much as possible *set* and *get* methods to add a security layer when we write or read class properties. So we can check the value that is going to be write in a property. For example `set_graph` method of the `GraphDataProcessor` class writes the RDF graph inside the property just if it is not empty, otherwise an error is printed in console.\n",
    "\n",
    "#### Terminal messages\n",
    "To be able to know at which step our program is during the execution, we made use of terminal messages. We divided these messages in three main categories. `INFO` messages updated the user about the step currently starting or just finished. `ERR` messages are alerts that report where the app failed its execution and stop while `WARN` was used to indicate when a query return zero results.\n",
    "\n",
    "#### Case formats\n",
    "We tried to use different case formats for indicate the different parts of our code. More in detail we used:\n",
    "- Pascal Case (PascalCase) for naming classe (e.g. `IdentifiableEntity`, `Person`, etc…)\n",
    "- Camel case (camelCase) for the class methods (e.g. `getIds`, `getGivenName`, etc…)\n",
    "- Snake case (snake_case), with an additional initial \"_\" for the functions arguments (e.g. `_id_list`, `_url`, etc…). Snake case was also used for namining some generic functions like those in the `common_utils.py` file (e.g. `csv_to_df`, `json_to_df`, etc…) and for naming constants (e.g. `BASE_URL`, `DB_PATH`, etc…)\n",
    "\n",
    "#### Constants names\n",
    "In order to easly read when a constat is used, we explicitly used a all capitals letter syntax.\n",
    "E.g. We will write the datasets paths as_\n",
    "\n",
    "    GRAPH_CSV_FILE; GRAPH_JSON_FILE; RELATIONAL_CSV_FILE; RELATIONAL_JSON_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Let's start\n",
    "We started by analysing the data that has been provided in order to understand the different cases we had to handle and their characteristics.  We have then created two data processors that we will use to extract data from the datasets provided and we will process them for the relational database and for the graph one.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_description'></a>\n",
    "\n",
    "# 2. Data description\n",
    "\n",
    "We started by analysing the exemplar JSON and CSV files that we have been provided with to test the software.\n",
    "The CSV files, both the *relational_publications.csv* and the *graph_publications.csv*, are composed by the following columns:\n",
    "\n",
    "    id, title, type, publication_year, issue, volume, chapter, publication_venue, venue_type, publisher, event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "title\n",
      "type\n",
      "publication_year\n",
      "issue\n",
      "volume\n",
      "chapter\n",
      "publication_venue\n",
      "venue_type\n",
      "publisher\n",
      "event\n"
     ]
    }
   ],
   "source": [
    "relational_csv = read_csv(\"data/relational_publications.csv\")\n",
    "for columns in relational_csv:\n",
    "    print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "title\n",
      "type\n",
      "publication_year\n",
      "issue\n",
      "volume\n",
      "chapter\n",
      "publication_venue\n",
      "venue_type\n",
      "publisher\n",
      "event\n"
     ]
    }
   ],
   "source": [
    "graph_csv = read_csv(\"data/graph_publications.csv\")\n",
    "for columns in graph_csv:\n",
    "    print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row defines a publication entity.\n",
    "As defined in the given UML, we will have three type of pubblications: journal articles, book chapters and proceedings papers.\n",
    "Journal articles can also have issue and volume specified, while book chapters must have a chapter number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![datamodel](software-documentation/img/datamodel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the JSON files, *relational_other_data.json* and *relational_other_data.json*, we will find additional informations about the publications and their related classes. In particular the JSON files are structured in 4 main keys:\n",
    "\n",
    "    authors, venues_id, references, publishers\n",
    "\n",
    "The first three sections contain additional information about authors, venues and citations of other publications by means of the publications' unique identifiers (DOI) used as sub-key inside each of these three macro \"dictionaries\". The fourth key give further information about the publishers that can be connected to the information of our csv through the mediation of their crossref identifier, which is used as key inside the json.\n",
    "\n",
    "The files have been analysed both manually and by means of Python in order to better understand both the quantitative and the qualitative characteristics fo the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors\n",
      "venues_id\n",
      "references\n",
      "publishers\n"
     ]
    }
   ],
   "source": [
    "relational_json = open(\"data/relational_other_data.json\", 'r', encoding='utf-8')\n",
    "\n",
    "relational_json_df = load(relational_json)\n",
    "for columns in relational_json_df:\n",
    "    print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors\n",
      "venues_id\n",
      "references\n",
      "publishers\n"
     ]
    }
   ],
   "source": [
    "graph_json = open(\"data/graph_other_data.json\", 'r', encoding='utf-8')\n",
    "\n",
    "graph_json_df = load(graph_json)\n",
    "for columns in graph_json_df:\n",
    "    print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative characteristcs of datasets.\n",
    "\n",
    "#### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect how the datasets are compose trough the *info* pandas method after reading into Python the CSVs. In addition we can use the *head* method to look the first 5 rows of our CSV tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 500 non-null    object \n",
      " 1   title              500 non-null    object \n",
      " 2   type               500 non-null    object \n",
      " 3   publication_year   500 non-null    int64  \n",
      " 4   issue              347 non-null    object \n",
      " 5   volume             443 non-null    object \n",
      " 6   chapter            22 non-null     float64\n",
      " 7   publication_venue  498 non-null    object \n",
      " 8   venue_type         498 non-null    object \n",
      " 9   publisher          498 non-null    object \n",
      " 10  event              0 non-null      float64\n",
      "dtypes: float64(2), int64(1), object(8)\n",
      "memory usage: 43.1+ KB\n"
     ]
    }
   ],
   "source": [
    "relational_publications = read_csv(\"data/relational_publications.csv\")\n",
    "relational_publications.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>issue</th>\n",
       "      <th>volume</th>\n",
       "      <th>chapter</th>\n",
       "      <th>publication_venue</th>\n",
       "      <th>venue_type</th>\n",
       "      <th>publisher</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doi:10.1162/qss_a_00023</td>\n",
       "      <td>Opencitations, An Infrastructure Organization ...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quantitative Science Studies</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:281</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doi:10.1007/s11192-019-03217-6</td>\n",
       "      <td>Software Review: Coci, The Opencitations Index...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scientometrics</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:297</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doi:10.1007/s11192-019-03311-9</td>\n",
       "      <td>Nine Million Book Items And Eleven Million Cit...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scientometrics</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:297</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doi:10.1038/sdata.2016.18</td>\n",
       "      <td>The Fair Guiding Principles For Scientific Dat...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scientific Data</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:297</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doi:10.1371/journal.pbio.3000385</td>\n",
       "      <td>The Nih Open Citation Collection: A Public Acc...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plos Biology</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:340</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0           doi:10.1162/qss_a_00023   \n",
       "1    doi:10.1007/s11192-019-03217-6   \n",
       "2    doi:10.1007/s11192-019-03311-9   \n",
       "3         doi:10.1038/sdata.2016.18   \n",
       "4  doi:10.1371/journal.pbio.3000385   \n",
       "\n",
       "                                               title             type  \\\n",
       "0  Opencitations, An Infrastructure Organization ...  journal-article   \n",
       "1  Software Review: Coci, The Opencitations Index...  journal-article   \n",
       "2  Nine Million Book Items And Eleven Million Cit...  journal-article   \n",
       "3  The Fair Guiding Principles For Scientific Dat...  journal-article   \n",
       "4  The Nih Open Citation Collection: A Public Acc...  journal-article   \n",
       "\n",
       "   publication_year issue volume  chapter             publication_venue  \\\n",
       "0              2020     1      1      NaN  Quantitative Science Studies   \n",
       "1              2019     2    121      NaN                Scientometrics   \n",
       "2              2019     2    122      NaN                Scientometrics   \n",
       "3              2016     1      3      NaN               Scientific Data   \n",
       "4              2019    10     17      NaN                  Plos Biology   \n",
       "\n",
       "  venue_type     publisher  event  \n",
       "0    journal  crossref:281    NaN  \n",
       "1    journal  crossref:297    NaN  \n",
       "2    journal  crossref:297    NaN  \n",
       "3    journal  crossref:297    NaN  \n",
       "4    journal  crossref:340    NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relational_publications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 500 non-null    object \n",
      " 1   title              500 non-null    object \n",
      " 2   type               500 non-null    object \n",
      " 3   publication_year   500 non-null    int64  \n",
      " 4   issue              303 non-null    object \n",
      " 5   volume             391 non-null    object \n",
      " 6   chapter            93 non-null     float64\n",
      " 7   publication_venue  486 non-null    object \n",
      " 8   venue_type         486 non-null    object \n",
      " 9   publisher          486 non-null    object \n",
      " 10  event              0 non-null      float64\n",
      "dtypes: float64(2), int64(1), object(8)\n",
      "memory usage: 43.1+ KB\n"
     ]
    }
   ],
   "source": [
    "graph_publication = read_csv(\"data/graph_publications.csv\")\n",
    "graph_publication.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>issue</th>\n",
       "      <th>volume</th>\n",
       "      <th>chapter</th>\n",
       "      <th>publication_venue</th>\n",
       "      <th>venue_type</th>\n",
       "      <th>publisher</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doi:10.1016/j.websem.2021.100655</td>\n",
       "      <td>Crossing The Chasm Between Ontology Engineerin...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Journal Of Web Semantics</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doi:10.1007/s10115-017-1100-y</td>\n",
       "      <td>Core Techniques Of Question Answering Systems ...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Knowledge And Information Systems</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:297</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doi:10.1016/j.websem.2014.03.003</td>\n",
       "      <td>Api-Centric Linked Data Integration: The Open ...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Journal Of Web Semantics</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doi:10.1093/nar/gkz997</td>\n",
       "      <td>The Monarch Initiative In 2019: An Integrative...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2019</td>\n",
       "      <td>D1</td>\n",
       "      <td>48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nucleic Acids Research</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:286</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doi:10.3390/publications7030050</td>\n",
       "      <td>Dras-Tic Linked Data: Evenly Distributing The ...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Publications</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:1968</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  doi:10.1016/j.websem.2021.100655   \n",
       "1     doi:10.1007/s10115-017-1100-y   \n",
       "2  doi:10.1016/j.websem.2014.03.003   \n",
       "3            doi:10.1093/nar/gkz997   \n",
       "4   doi:10.3390/publications7030050   \n",
       "\n",
       "                                               title             type  \\\n",
       "0  Crossing The Chasm Between Ontology Engineerin...  journal-article   \n",
       "1  Core Techniques Of Question Answering Systems ...  journal-article   \n",
       "2  Api-Centric Linked Data Integration: The Open ...  journal-article   \n",
       "3  The Monarch Initiative In 2019: An Integrative...  journal-article   \n",
       "4  Dras-Tic Linked Data: Evenly Distributing The ...  journal-article   \n",
       "\n",
       "   publication_year issue volume  chapter                  publication_venue  \\\n",
       "0              2021   NaN     70      NaN           Journal Of Web Semantics   \n",
       "1              2017     3     55      NaN  Knowledge And Information Systems   \n",
       "2              2014   NaN     29      NaN           Journal Of Web Semantics   \n",
       "3              2019    D1     48      NaN             Nucleic Acids Research   \n",
       "4              2019     3      7      NaN                       Publications   \n",
       "\n",
       "  venue_type      publisher  event  \n",
       "0    journal    crossref:78    NaN  \n",
       "1    journal   crossref:297    NaN  \n",
       "2    journal    crossref:78    NaN  \n",
       "3    journal   crossref:286    NaN  \n",
       "4    journal  crossref:1968    NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_publication.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in this first exploration of the two CSV provided, we already see some quantitative difference between the two datasets. This lead to the conclusion that the two databases we will create cuold have different informations about different publications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 540 entries, doi:10.1162/qss_a_00023 to crossref:301\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   authors     508 non-null    object\n",
      " 1   venues_id   498 non-null    object\n",
      " 2   references  500 non-null    object\n",
      " 3   publishers  32 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 21.1+ KB\n"
     ]
    }
   ],
   "source": [
    "relational_other_data = read_json(\"data/relational_other_data.json\")\n",
    "relational_other_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>venues_id</th>\n",
       "      <th>references</th>\n",
       "      <th>publishers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>508</td>\n",
       "      <td>498</td>\n",
       "      <td>500</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>486</td>\n",
       "      <td>297</td>\n",
       "      <td>99</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>[{'family': 'Leydesdorff', 'given': 'Loet', 'o...</td>\n",
       "      <td>[issn:0138-9130, issn:1588-2861]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'id': 'crossref:6228', 'name': 'Codon Publica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>366</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  authors  \\\n",
       "count                                                 508   \n",
       "unique                                                486   \n",
       "top     [{'family': 'Leydesdorff', 'given': 'Loet', 'o...   \n",
       "freq                                                    4   \n",
       "\n",
       "                               venues_id references  \\\n",
       "count                                498        500   \n",
       "unique                               297         99   \n",
       "top     [issn:0138-9130, issn:1588-2861]         []   \n",
       "freq                                  50        366   \n",
       "\n",
       "                                               publishers  \n",
       "count                                                  32  \n",
       "unique                                                 32  \n",
       "top     {'id': 'crossref:6228', 'name': 'Codon Publica...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relational_other_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 563 entries, doi:10.1016/j.websem.2021.100655 to crossref:4443\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   authors     526 non-null    object\n",
      " 1   venues_id   486 non-null    object\n",
      " 2   references  500 non-null    object\n",
      " 3   publishers  37 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 22.0+ KB\n"
     ]
    }
   ],
   "source": [
    "graph_other_data = read_json(\"./data/graph_other_data.json\")\n",
    "graph_other_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>venues_id</th>\n",
       "      <th>references</th>\n",
       "      <th>publishers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>526</td>\n",
       "      <td>486</td>\n",
       "      <td>500</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>492</td>\n",
       "      <td>309</td>\n",
       "      <td>101</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>[{'family': 'Pal', 'given': 'Kamalendu', 'orci...</td>\n",
       "      <td>[issn:2076-3417]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'id': 'crossref:735', 'name': 'Thomas Telford...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>331</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  authors         venues_id  \\\n",
       "count                                                 526               486   \n",
       "unique                                                492               309   \n",
       "top     [{'family': 'Pal', 'given': 'Kamalendu', 'orci...  [issn:2076-3417]   \n",
       "freq                                                    4                15   \n",
       "\n",
       "       references                                         publishers  \n",
       "count         500                                                 37  \n",
       "unique        101                                                 37  \n",
       "top            []  {'id': 'crossref:735', 'name': 'Thomas Telford...  \n",
       "freq          331                                                  1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_other_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the CSVs, also for the data provided inside the JSONs, we found some quantitative difference.\n",
    "\n",
    "Since the final outputs of the software must be Python objects that reflects the data stored in both databases, we will have to check for common entries and diffent ones. We kept this in mind while creating the generic query processor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='software_organization'></a>\n",
    "\n",
    "# 3. Software organization\n",
    "\n",
    "We try to organize the software to be scalable and accessible.\n",
    "It is scalable since we tried to keep the code as general-purpose as possible. The idea is to have a basic structure that can be adapted to different datasets or future implementation of different kind of databases.\n",
    "It is accessible since we designed an **entry point, `main.py`,** foreasly access the software. This feature was also important in the testing and debugging phases.\n",
    "\n",
    "Let's take a look at all the files contained in the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"requirements.txt\"></div>\n",
    "\n",
    "## requirements.txt\n",
    "\n",
    "Python requirements files are a great way to keep track of the Python modules. It is a simple text file that saves a list of the modules and packages required by your project. By creating a Python requirements.txt file, you save yourself the hassle of having to track down and install all of the required modules manually.\n",
    "\n",
    "It makes it easy to share your project with others. They install the same Python modules you have listed in your requirements file and run your project without any problems.\n",
    "\n",
    "In case you ever need to update or add a Python module to your project, you simply update the requirements file rather than having to search through all of your code for every reference to the old module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"const.py\"></div>\n",
    "\n",
    "## const.py\n",
    "\n",
    "In this file we stored all the constants we will need in ou project. This feature allows to easly change the value of elements widly used in the execution. For example you will find the data source local path, the base url of our RDF resources we will create, the path where the relational database file will be stored, and so on.\n",
    "\n",
    "Regarding the triplestore database, we also decide to store here the queries we will use in the `TriplestoreQueryProcessor`, this allows to have a more clear code in the query process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"URIs.py\"></div>\n",
    "\n",
    "## URIs.py\n",
    "\n",
    "The [*Resource Description Framework* (RDF)](https://www.w3.org/RDF/) allows users to describe both Web documents and concepts from the real world—people, organisations, topics, things—in a computer-processable way. Publishing such descriptions on the Web creates the Semantic Web. [URIs (*Uniform Resource Identifiers*)](https://www.w3.org/Addressing/URL/uri-spec.html) are very important, providing both the core of the framework itself and the link between RDF and the Web.\n",
    "\n",
    "To have them all in one place we created a dedicate file. All classes of resources and proprieters that relates them, defined by the UML provided, will be presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"data_model.py\"></div>\n",
    "\n",
    "## data_model.py\n",
    "\n",
    "The structure presented in UML is translated in this file.\n",
    "\n",
    "![datamodel](software-documentation/img/datamodel.png)\n",
    "\n",
    "We define all the Python classes and relative sub-classes.\n",
    "This process is import in order to return Python objects from the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div id=\"common_utils.py\"></div>\n",
    "\n",
    "## common_utils.py\n",
    "\n",
    "Here you can find general function that we used in the programm. They address specific task we need to perform during the execution. You can find in this file custom functions like: `csv_to_df`, `json_to_df` or `blazegraph_instance_is_active` that check trough a HTTP request is the Blazegraph service is active or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"main.py\"></div>\n",
    "\n",
    "## main.py\n",
    "\n",
    "This is the entry point of our program, we launch all the processes we need to obtain our outputs, all wrapped in an `app` function.\n",
    "\n",
    "A key aspect for devolp a complex program like this one was to allow the possibility to run debugs.\n",
    "Since we used Visual Studio Core as editor, we took advantage of one of the key features of this editor, its great debugging support. VS Code's built-in debugger helps accelerate the edit, compile, and debug loop.\n",
    "To be able to do so we setted up the `launch.json` (contained in the `.vscode` folder) file as reported in [this guide by VS Code](https://code.visualstudio.com/docs/editor/debugging).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"data_processors.py\"></div>\n",
    "\n",
    "## data_processors.py\n",
    "\n",
    "In this file we process all the data provided and we build the [pandas DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) we will need for populate our databases.\n",
    "\n",
    "We created a custom class `DataProcessor` that will take as attributes all the DataFrames we will create. As made in other cases, when we need to write or read attributes of a class, we used the respective *set* and *get* methods for each attribute.\n",
    "\n",
    "You can also find the costum `DataProcessor`'s sub-class we created, `GraphDataProcessor`, where we compose all the triplets of our graph database. As properties of this class we setted all the dictonaries we used in the process to create the relation between the different entities we describe. \n",
    "\n",
    "In the original design of the program we had imagined to perform in this file also the creation of the tables for the relational database. In the final design presented here, we instead splited the creation of the tables for the relational database in another file that we will see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"relational_processor.py\"></div>\n",
    "\n",
    "## relational_processor.py\n",
    "\n",
    "The upload data function in relational data processors are divided into 2 parts. when we upload csv data we create all tables required for both csv and json. for csv data we populate these: Book, BookChapter, Journal, JournalArticles, PublicationID. Proceeeding and ProceedingPaper are empty according to our data.\n",
    "\n",
    "We also create these empty tables to make it ready to use for json data in next step:\n",
    "Author, Cites, Organization, OrgID, Person, PersonID, and VenueID.\n",
    "\n",
    "With json data, references are linked to publications. Moreover, venue ids is linked to venues. Additionaly, authors and organizations are linked to publications and venues.\n",
    "\n",
    "For creating tables for each publication type we merge citations, authors and venues.\n",
    "\n",
    "In order to link publications to authors, we create author table with dois and personID table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"triplestore_processor.py\"></div>\n",
    "\n",
    "## triplestore_processor.py\n",
    "\n",
    "After the creation of the triplets in the `data_processor.py`, we upload them to our online service, Blazegraph, to be stored and to have a queryable endpoint.\n",
    "\n",
    "The base class `TriplestoreProcessor` sotres in the variable `endpointUrl` the URL of the SPARQL endpoint of the triplestore, initially set as an empty string, that will be updated with the method `setEndpointUrl`\n",
    "\n",
    "The sub-class `TriplestoreDataProcessor`, with its method `uploadData` enables to upload the collection of data specified in the input file path into the database. We check if the file in input is a CSV or JSON and we launch the methods for the creation of the different DataFrames, already created in the `GraphDataProcessor`. Before this step we step if the data are already been uploaded in a precedent execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"query_processors.py\"></div>\n",
    "\n",
    "## query_processors.py\n",
    "\n",
    "Two main classes will be contained in this file: `RelationalQueryProcessor` and `TriplestoreQueryProcessor`. Both classes will have as methods the queries required in the project guidelines. As designed in the \"UML of additional classes\", these two classes will be sub-classes of the respective processor classes (`RelationalProcessor` and `TriplestoreProcessor`) and also both sub-classes of a generic `QueryProcessor`.\n",
    "\n",
    "In this same file we clean the DataFrames returning from each query processor and we combine the two in one DataFrame with the [pandas concat method.](https://pandas.pydata.org/docs/reference/api/pandas.concat.html?highlight=concat#pandas.concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"queries.py\"></div>\n",
    "\n",
    "## query.py\n",
    "\n",
    "We created this costum file to manage the execution of all the queries. From the `main.py` we launche the execution of the function here contained. All the functions here contained will take as input the list of processor we have in our project (this feature allows to add in future also another data processor if needed, like a NoSQL processor for example), after the execution of the generic query, we process the result and translate them into Python object defined in the `data_model.py`. At least we write a *txt* file for each query, these files will be stored in the `queries-results` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"functioning\"></div>\n",
    "\n",
    "# 4.Functioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functioning of the programm can be resumed in the following image:\n",
    "\n",
    "![workflow](software-documentation/img/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We start from *main.py*\n",
    "\n",
    "The execution of our program starts from the entry point, setted in the `launch.json` file, the `main.py`. The app starts runnining and the first message appears in console to confirm that. \n",
    "\n",
    "The `app` function contains the whole program. As already said, this design, helps create a more clear sequence of steps that the program needs to execute to achive our final goal.\n",
    "\n",
    "Since we will use few external libraries to handle specific tasks, we insert the execution of `app` in a *try/except* statement. We do that to be able to understand if and when an error is produced during the execution, above all if the error occur in a part of the code out of our program, an external library for example. \n",
    "\n",
    "As first thing we see a flag `data_has_been_uploaded` setted by default at *False*. We will need that more further on to check if the data have been correctly updated on Blazegraph.\n",
    "\n",
    "As already mentioned Blazegraph is the web service we used for upload our graph database.\n",
    "The function `blazegraph_instance_is_active()` perform an HTTP request, using a method of the exeternal library [*request*](https://pypi.org/project/requests/), and checks if Blazegraph is already connected, otherwise we start the connection with `start_blazegraph_server()`. Once the connection is succesfully accomplished, we check if the graph database was already populated by a previous execution of the program with `blazegraph_instance_is_empty()`. A test query has been set up to check if any triples is inside the Blazegraph local endpoint (http://127.0.0.1:9999/blazegraph/sparql)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's process our datasets\n",
    "\n",
    "### Triplestore data processor\n",
    "If Blazegraph service correctly started and the Blazegraph instance results empty, we can run our data processors.\n",
    "\n",
    "We start from the `TriplestoreDataProcessor()`. Instatiating the class we now have access to \n",
    "its methods. The endpoint is setted with `setEndpointUrl()`, method inherited from the super-class `TriplestoreProcessor()`. \n",
    "To elaborate all the DataFrames we weill need we created a custom class `GraphDataProcessor`. This class will populate the property *processor* of `TriplestoreDataProcessor` class with all those DataFrames.\n",
    "\n",
    "The method `uploadData` will check into the datasets provided which of them contains the word \"graph\" in the name. If the files names are valid, we check if they are CSV or JSON. From our initial analysis of the datasets provided, we know that the CSV files contain informations about the Publications, while the JSON files contain informations about Authors, Publishers, References and Venues.\n",
    "The *processor* previously initialised allows us to use the method `publicationsDfBuilder` of `DataProcessor`class, so we can use it to write the new DataFrame of the Publications created from the CSV file as proporty of the `DataProcessor` class. As already said this latter class has as properties all the DataFrames we will need to process the data.\n",
    "Ater this, we run the DataFrame builder methods (`authorsDfBuilder`, `publishersDfBuilder` , `referncesDfBuilder` and `venuesDfBuilder`)on the JSON file.\n",
    "\n",
    "### Creation of RDF triples\n",
    "An RDF triple contains three components:\n",
    "- the subject, which is an RDF URI reference or a blank node\n",
    "- the predicate, which is an RDF URI reference\n",
    "- the object, which is an RDF URI reference, a literal or a blank node\n",
    "An RDF triple is conventionally written in the order subject, predicate, object. The predicate is also known as the property of the triple.\n",
    "\n",
    "To work with RDF in Python we used the external library [RDFLib](https://rdflib.readthedocs.io/en/stable/).\n",
    "\n",
    "We now have all the DataFrames from which we will create our triplets. So, after all the DataFrames have been succesfully built and have been written as property of `DataProcessor` class, we can launch the `graphBuilder` method of `GraphDataProcessor`. In here the methods that actually make up the triplets are launched. So `do_organization_triples` will create all the Organizations entities, `do_venue_triples` will create all the Venues entities, `do_person_triplets` will be used to form the triplets relative to the Authors and `do_publication_triples` will be used for create all the triplets of all the Publications entities.\n",
    "\n",
    "While creating the triplets we describe the entities we are creating. They all will have as base url \"https://allorapy.github.io/res/\" plus the name of the entity (e.g. \"publication-0\"). The final URI of the resource will be than composed (e.g. \"https://allorapy.github.io/res/publication-0\") and it will be the subject of our triplets. As predicate we will assign the properties defined in the `URIs.py` file, where wi will use different ontologies to describe our resources. As object of the triplet we will associate, depending from the design of the resource, a literal or another entity.\n",
    "\n",
    "An RDF graph is a set of RDF triples. The set of nodes of an RDF graph is the set of subjects and objects of triples in the graph. All the triples created are added to our graph (*\"rdf_graph\"*) through the `add` method of the RDFLib external library.\n",
    "\n",
    "### Graph deploying\n",
    "\n",
    "The deploy on the web endpoint of our graph is endeled by the method `triplestoreDeploy` of our `GraphDataProcessor` class. We initialize a `SPARQLUpdateStore` as defined in the [RDFLib sparqlstore extension documentation](https://github.com/RDFLib/rdflib-sparqlstore/blob/master/README.md). The RDFLib method `open` opens the connection with the SPARQL endpoint instance.\n",
    "\n",
    "Each triplet in our graph will be added to the online database. When we are done we close the connection with the `close` method.\n",
    "\n",
    "Our flag `data_has_been_uploaded` will be setted at *True* if the operation is executed without errors.\n",
    "\n",
    "#### *Problems\n",
    "\n",
    "We encountered a bug during the deploy of the graph. We saw that the UTF-8 enconding is not respected. So, while the data are correctly read from the CSV or JSON, they are changed when uploaded in Blazegraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the SPARQL endpoint\n",
    "\n",
    "Now we can perform our queries.\n",
    "\n",
    "We instantiate both the query processors, `TriplestoreQueryProcessor` and `RelationalQueryProcessor`.\n",
    "The instances of these two classes qill be insert in a list of processors.\n",
    "We than create a `GenericQueryProcessor` object and we add the previous created list to it through the `addQueryProcessor` method. This method will return a *boolean* value after the add.\n",
    "If all processors are succesfully added, *True* is returned. In case we would have a third database, the `GenericQueryProcessor` object could contain them with no problem since the list of processors is passed as input to `addQueryProcessor` and then written in the property of the class.\n",
    "\n",
    "The following query methods are contained in the `GenericQueryProcessor` object and not in the single processors because the queries are the same for all the processors but just written in different languages (SQL for relational adn SPARQL for triplestore).\n",
    "Each query is called just one time but performs different actions depending from the processors in the list.\n",
    "\n",
    "The SQL and the SPARQL queries return a DataFrame with the answer. Since the different structure of the two databases, operations of cleaning and matching of the result DataFrame are performed. We can now combine these DataFrames in one with the same columns.\n",
    "\n",
    "Now we can create the objects compliant with the data model provided and we append them in a list.\n",
    "We can now return Python objects from the queries.\n",
    "\n",
    "I due metodi relativi a relational e graph restituiscono i data frame di risultato. Una volta uniti i due dataframe di risultato dei due database, viene fatta una lista e restituiamo così un Python object.\n",
    "La stessa lociga è applicata a tutte le query. \n",
    "\n",
    "Messages in console will be printed to conferm the execution of the queries and a *txt* files with the results will be written in the queries-results folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational Query Processor\n",
    "\n",
    "In the relational query processor class we have different modules and we take as input the relational processor.\n",
    "The structure of every function (i.e. module) is really similar.\n",
    "First of all we made the connection to the SQL database, where we have previously uploaded our data.\n",
    "Then we have 3 main parts: SELECT, FROM, WHERE. With SELECT we are going to decide the columns that we are interested in from a specific table and those we want to show in the output. The table we are considering is the one named in FROM part. In the WHERE section we filter our table for values (as rows) that should be returned as ouput.\n",
    "If we have stored our data in different tables of the database, to obtain a complete output, we have to JOIN those. In our modules it was done with a LEFT JOIN, to be sure of keeping all the data present in the table selected in the FROM section, althought it has no corrispondences in tables that are joined. A NAN value fill the white slots here.\n",
    "It is important keeping in mind that in every SELECT of every UNION part, we must have the same number of columns. Output of every part is going to be added at the bottom of the previous query.\n",
    "Outputs of queries of the relational query processor are dataframes as per guidelines. Those output are then shared with other parts of the code as the generic part.\n",
    "\n",
    "### Let's explain a module as example:\n",
    "getPublicationsByAuthorName: It returns a data frame with all the publications (i.e. the rows) that have been authored by the people having their name matching (in lowercase), even partially, with the name specified as input (e.g. \"doe\").\n",
    "\n",
    "Based on tables created on the database we should go from a string (the name of the author) or from a partial string to the publications that are authored from people that is named with this string.\n",
    "First of all, we know that we want all the publications under this condition, so since we have publications in 3 different tables (JournalArticles, BookChapter, ProceedingPaper) we should go to query all of them. We are going to to 3 different queries and then we'll join them through the UNION.\n",
    "So we set our first table to be queried in the FROM (JournalArticle), then we LEFT JOIN tables (bridge tables and tables that contains data we want to return). Author is here a table used as bridge because we need it only to connect JournalArticle to Person. We linked the two tables through the common column AuthorID (named author in the JournalArticle table). We decided to add also the PubID table to be able to return the publication id as identifier.\n",
    "We can now filter our 'virtual table' using the WHERE. We could have capital letters, so we have to clean a little data before going ahead. We lower all the letters of the given name and of the family name. We search with the WHERE instruction the strings that are equal (or partial equal adding % before) to the authorPartialName given by input.\n",
    "We have now a bigger table than we need (and want), so we are going to SELECT only the columns whose we are really interested in, keeping in mind that, if we want return a column that won't be present in the tables we are going to create for BookChapter and ProceedingPaper, we will have to set the column as NA in the SELECT of the table where it is absent.\n",
    "We repeat the process for BookChapter and ProceedingPaper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='descriptive_statistics'></a>\n",
    "\n",
    "# 5. Descriptive Statistics\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (v3.10.6:9c7b4bd164, Aug  1 2022, 17:13:48) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
