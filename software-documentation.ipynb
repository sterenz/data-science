{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External modules used in this documentation\n",
    "\n",
    "In this short code section we upload all the needed modules for this documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software documentation\n",
    "\n",
    "1. [Introduction](#introduction) \n",
    "2. [Data description](#data_description)\n",
    "3. [Software organization](#software_organization)\n",
    "    - [requirements.txt](#requirements.txt)\n",
    "    - [const.py](#const.py)\n",
    "    - [URIs.py](#URIs.py)\n",
    "    - [data_model.py](#data_model.py)\n",
    "    - [common_utils.py](#common_utils.py)\n",
    "    - [main.py](#main.py)\n",
    "    - [data_processors.py](#data_processors.py)\n",
    "    - [relational_processor.py](#relational_processor.py)\n",
    "    - [triplestore_processor.py](#triplestore_processor.py)\n",
    "    - [query_processors.py](#query_processors.py)\n",
    "    - [queries.py](#queries.py)\n",
    "    \n",
    "4. [Functioning](#functioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div id=introduction> </div>\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### Project goal\n",
    "Our goal is to build a software that enables to populate two kind of databases, a relational and a graph database, and to query these databases simultaneously.  \n",
    "In this project we will deal with *structured data* coming from CSV and JSON files. For processing the data in Python we will use [***pandas***](https://pandas.pydata.org/) , a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive.\n",
    "\n",
    "After shortly analysing the provided data and their characteristics, we developed the processors that will extract the data from the provided CSVs and JSONs, in order to populate our structured collection of data. We will than upload the records created into our databases, as well as providing the possibility to query these databases simultaneously and return specific Python objects.\n",
    "\n",
    "### Design and syntax choices\n",
    "\n",
    "#### Type annotation\n",
    "Since an important characteristic of the Python language is the consistency of its object model. Each object has an associated type (e.g., integer, string, or function) and internal data. We choose, for a better readability, to make the Data Types of the input parameters (arguments) and the output of the function explicit by the use of type annotation.\n",
    "E.g.  \n",
    "\n",
    "    def set_df(self, _df: DataFrame) -> None:\n",
    "\n",
    "This meas that `set_df` will take a DataFrame as input and will return anything, so None. Note, though, that this is syntactic sugar only, Python will not raise an error if the Data Type of the arguments specified in the annotation is not respected (e.g. string, integer, list,…).\n",
    "    \n",
    "#### Try/except statement\n",
    "We made use of the `try/except` statement in the code to handle errors. The `try/except` statement works as follows:\n",
    "\n",
    "- First, the try clause (the statement(s) between the try and except keywords) is executed.\n",
    "\n",
    "- If no exception occurs, the except clause is skipped and execution of the try statement is finished.\n",
    "\n",
    "- If an exception occurs during the execution of the try clause, the rest of the clause is skipped. Then, if its type matches the exception named after the except keyword, the except clause is executed, and then execution continues after the try/except block.\n",
    "\n",
    "Is it also possible to catch the specific Python error in the except statement with the build-in class `Exception` and print it in the terminal.\n",
    "\n",
    "#### Terminal messages\n",
    "To be able to know at which step our program is during the execution, we made use of terminal messages. We divided these messages in three main categories. `INFO` messages updated the user about the step currently starting or just finished. `ERR` messages are alerts that report where the app failed its execution and stop while `WARN` was used to indicate when a query return zero results.\n",
    "\n",
    "#### Case formats\n",
    "We tried to use different case formats for indicate the different parts of our code. More in detail we used:\n",
    "- Pascal Case (PascalCase) for naming classe (e.g. `IdentifiableEntity`, `Person`, etc…)\n",
    "- Camel case (camelCase) for the class methods (e.g. `getIds`, `getGivenName`, etc…)\n",
    "- Sanke case (snake_case), with an additional initial \"_\" for the arguments (e.g. `_id_list`, `_url`, etc…)\n",
    "\n",
    "#### Constants names\n",
    "In order to easly read when a constat is used, we explicitly used a all capitals letter syntax.\n",
    "E.g. We will write the datasets paths as_\n",
    "\n",
    "    GRAPH_CSV_FILE; GRAPH_JSON_FILE; RELATIONAL_CSV_FILE; RELATIONAL_JSON_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Let's start\n",
    "We started by analysing the data that has been provided in order to understand the different cases we had to handle and their characteristics.  We have then created two data processors that we will use to extract data from the datasets provided and we will process them for the relational database and for the graph one.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_description'></a>\n",
    "\n",
    "## 2. Data description\n",
    "\n",
    "We started by analysing the exemplar JSON and CSV files that we have been provided with to test the software.\n",
    "The CSV files, both the *relational_publications.csv* and the *graph_publications.csv*, are composed by the following columns:\n",
    "\n",
    "    id, title, type, publication_year, issue, volume, chapter, publication_venue, venue_type, publisher, event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "title\n",
      "type\n",
      "publication_year\n",
      "issue\n",
      "volume\n",
      "chapter\n",
      "publication_venue\n",
      "venue_type\n",
      "publisher\n",
      "event\n"
     ]
    }
   ],
   "source": [
    "relational_csv = read_csv(\"data/relational_publications.csv\")\n",
    "for columns in relational_csv:\n",
    "    print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "title\n",
      "type\n",
      "publication_year\n",
      "issue\n",
      "volume\n",
      "chapter\n",
      "publication_venue\n",
      "venue_type\n",
      "publisher\n",
      "event\n"
     ]
    }
   ],
   "source": [
    "graph_csv = read_csv(\"data/graph_publications.csv\")\n",
    "for columns in graph_csv:\n",
    "    print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row defines a publication entity.\n",
    "As defined in the given UML, we will have three type of pubblications: journal articles, book chapters and proceedings papers.\n",
    "Journal articles can also have issue and volume specified, while book chapters must have a chapter number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![datamodel](software-documentation/img/datamodel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the JSON files, *relational_other_data.json* and *relational_other_data.json*, we will find additional informations about the publications and their related classes. In particular the JSON files are structured in 4 main keys:\n",
    "\n",
    "    authors, venues_id, references, publishers\n",
    "\n",
    "The first three sections contain additional information about authors, venues and citations of other publications by means of the publications' unique identifiers (DOI) used as sub-key inside each of these three macro \"dictionaries\". The fourth key give further information about the publishers that can be connected to the information of our csv through the mediation of their crossref identifier, which is used as key inside the json.\n",
    "\n",
    "The files have been analysed both manually and by means of Python in order to better understand both the quantitative and the qualitative characteristics fo the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors\n",
      "venues_id\n",
      "references\n",
      "publishers\n"
     ]
    }
   ],
   "source": [
    "relational_json = open(\"data/relational_other_data.json\", 'r', encoding='utf-8')\n",
    "\n",
    "relational_json_df = load(relational_json)\n",
    "for columns in relational_json_df:\n",
    "    print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors\n",
      "venues_id\n",
      "references\n",
      "publishers\n"
     ]
    }
   ],
   "source": [
    "graph_json = open(\"data/graph_other_data.json\", 'r', encoding='utf-8')\n",
    "\n",
    "graph_json_df = load(graph_json)\n",
    "for columns in graph_json_df:\n",
    "    print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative characteristcs of datasets.\n",
    "\n",
    "#### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect how the datasets are compose trough the *info* pandas method after reading into Python the CSVs. In addition we can use the *head* method to look the first 5 rows of our CSV tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 500 non-null    object \n",
      " 1   title              500 non-null    object \n",
      " 2   type               500 non-null    object \n",
      " 3   publication_year   500 non-null    int64  \n",
      " 4   issue              347 non-null    object \n",
      " 5   volume             443 non-null    object \n",
      " 6   chapter            22 non-null     float64\n",
      " 7   publication_venue  498 non-null    object \n",
      " 8   venue_type         498 non-null    object \n",
      " 9   publisher          498 non-null    object \n",
      " 10  event              0 non-null      float64\n",
      "dtypes: float64(2), int64(1), object(8)\n",
      "memory usage: 43.1+ KB\n"
     ]
    }
   ],
   "source": [
    "relational_publications = read_csv(\"data/relational_publications.csv\")\n",
    "relational_publications.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>issue</th>\n",
       "      <th>volume</th>\n",
       "      <th>chapter</th>\n",
       "      <th>publication_venue</th>\n",
       "      <th>venue_type</th>\n",
       "      <th>publisher</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doi:10.1162/qss_a_00023</td>\n",
       "      <td>Opencitations, An Infrastructure Organization ...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quantitative Science Studies</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:281</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doi:10.1007/s11192-019-03217-6</td>\n",
       "      <td>Software Review: Coci, The Opencitations Index...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scientometrics</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:297</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doi:10.1007/s11192-019-03311-9</td>\n",
       "      <td>Nine Million Book Items And Eleven Million Cit...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scientometrics</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:297</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doi:10.1038/sdata.2016.18</td>\n",
       "      <td>The Fair Guiding Principles For Scientific Dat...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scientific Data</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:297</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doi:10.1371/journal.pbio.3000385</td>\n",
       "      <td>The Nih Open Citation Collection: A Public Acc...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plos Biology</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:340</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0           doi:10.1162/qss_a_00023   \n",
       "1    doi:10.1007/s11192-019-03217-6   \n",
       "2    doi:10.1007/s11192-019-03311-9   \n",
       "3         doi:10.1038/sdata.2016.18   \n",
       "4  doi:10.1371/journal.pbio.3000385   \n",
       "\n",
       "                                               title             type  \\\n",
       "0  Opencitations, An Infrastructure Organization ...  journal-article   \n",
       "1  Software Review: Coci, The Opencitations Index...  journal-article   \n",
       "2  Nine Million Book Items And Eleven Million Cit...  journal-article   \n",
       "3  The Fair Guiding Principles For Scientific Dat...  journal-article   \n",
       "4  The Nih Open Citation Collection: A Public Acc...  journal-article   \n",
       "\n",
       "   publication_year issue volume  chapter             publication_venue  \\\n",
       "0              2020     1      1      NaN  Quantitative Science Studies   \n",
       "1              2019     2    121      NaN                Scientometrics   \n",
       "2              2019     2    122      NaN                Scientometrics   \n",
       "3              2016     1      3      NaN               Scientific Data   \n",
       "4              2019    10     17      NaN                  Plos Biology   \n",
       "\n",
       "  venue_type     publisher  event  \n",
       "0    journal  crossref:281    NaN  \n",
       "1    journal  crossref:297    NaN  \n",
       "2    journal  crossref:297    NaN  \n",
       "3    journal  crossref:297    NaN  \n",
       "4    journal  crossref:340    NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relational_publications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 500 non-null    object \n",
      " 1   title              500 non-null    object \n",
      " 2   type               500 non-null    object \n",
      " 3   publication_year   500 non-null    int64  \n",
      " 4   issue              303 non-null    object \n",
      " 5   volume             391 non-null    object \n",
      " 6   chapter            93 non-null     float64\n",
      " 7   publication_venue  486 non-null    object \n",
      " 8   venue_type         486 non-null    object \n",
      " 9   publisher          486 non-null    object \n",
      " 10  event              0 non-null      float64\n",
      "dtypes: float64(2), int64(1), object(8)\n",
      "memory usage: 43.1+ KB\n"
     ]
    }
   ],
   "source": [
    "graph_publication = read_csv(\"data/graph_publications.csv\")\n",
    "graph_publication.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>issue</th>\n",
       "      <th>volume</th>\n",
       "      <th>chapter</th>\n",
       "      <th>publication_venue</th>\n",
       "      <th>venue_type</th>\n",
       "      <th>publisher</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doi:10.1016/j.websem.2021.100655</td>\n",
       "      <td>Crossing The Chasm Between Ontology Engineerin...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Journal Of Web Semantics</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doi:10.1007/s10115-017-1100-y</td>\n",
       "      <td>Core Techniques Of Question Answering Systems ...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Knowledge And Information Systems</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:297</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doi:10.1016/j.websem.2014.03.003</td>\n",
       "      <td>Api-Centric Linked Data Integration: The Open ...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Journal Of Web Semantics</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doi:10.1093/nar/gkz997</td>\n",
       "      <td>The Monarch Initiative In 2019: An Integrative...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2019</td>\n",
       "      <td>D1</td>\n",
       "      <td>48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nucleic Acids Research</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:286</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doi:10.3390/publications7030050</td>\n",
       "      <td>Dras-Tic Linked Data: Evenly Distributing The ...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Publications</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:1968</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  doi:10.1016/j.websem.2021.100655   \n",
       "1     doi:10.1007/s10115-017-1100-y   \n",
       "2  doi:10.1016/j.websem.2014.03.003   \n",
       "3            doi:10.1093/nar/gkz997   \n",
       "4   doi:10.3390/publications7030050   \n",
       "\n",
       "                                               title             type  \\\n",
       "0  Crossing The Chasm Between Ontology Engineerin...  journal-article   \n",
       "1  Core Techniques Of Question Answering Systems ...  journal-article   \n",
       "2  Api-Centric Linked Data Integration: The Open ...  journal-article   \n",
       "3  The Monarch Initiative In 2019: An Integrative...  journal-article   \n",
       "4  Dras-Tic Linked Data: Evenly Distributing The ...  journal-article   \n",
       "\n",
       "   publication_year issue volume  chapter                  publication_venue  \\\n",
       "0              2021   NaN     70      NaN           Journal Of Web Semantics   \n",
       "1              2017     3     55      NaN  Knowledge And Information Systems   \n",
       "2              2014   NaN     29      NaN           Journal Of Web Semantics   \n",
       "3              2019    D1     48      NaN             Nucleic Acids Research   \n",
       "4              2019     3      7      NaN                       Publications   \n",
       "\n",
       "  venue_type      publisher  event  \n",
       "0    journal    crossref:78    NaN  \n",
       "1    journal   crossref:297    NaN  \n",
       "2    journal    crossref:78    NaN  \n",
       "3    journal   crossref:286    NaN  \n",
       "4    journal  crossref:1968    NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_publication.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in this first exploration of the two CSV provided, we already see some quantitative difference between the two datasets. This lead to the conclusion that the two databases we will create cuold have different informations about different publications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 540 entries, doi:10.1162/qss_a_00023 to crossref:301\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   authors     508 non-null    object\n",
      " 1   venues_id   498 non-null    object\n",
      " 2   references  500 non-null    object\n",
      " 3   publishers  32 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 21.1+ KB\n"
     ]
    }
   ],
   "source": [
    "relational_other_data = read_json(\"data/relational_other_data.json\")\n",
    "relational_other_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>venues_id</th>\n",
       "      <th>references</th>\n",
       "      <th>publishers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>508</td>\n",
       "      <td>498</td>\n",
       "      <td>500</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>486</td>\n",
       "      <td>297</td>\n",
       "      <td>99</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>[{'family': 'Leydesdorff', 'given': 'Loet', 'o...</td>\n",
       "      <td>[issn:0138-9130, issn:1588-2861]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'id': 'crossref:6228', 'name': 'Codon Publica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>366</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  authors  \\\n",
       "count                                                 508   \n",
       "unique                                                486   \n",
       "top     [{'family': 'Leydesdorff', 'given': 'Loet', 'o...   \n",
       "freq                                                    4   \n",
       "\n",
       "                               venues_id references  \\\n",
       "count                                498        500   \n",
       "unique                               297         99   \n",
       "top     [issn:0138-9130, issn:1588-2861]         []   \n",
       "freq                                  50        366   \n",
       "\n",
       "                                               publishers  \n",
       "count                                                  32  \n",
       "unique                                                 32  \n",
       "top     {'id': 'crossref:6228', 'name': 'Codon Publica...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relational_other_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 563 entries, doi:10.1016/j.websem.2021.100655 to crossref:4443\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   authors     526 non-null    object\n",
      " 1   venues_id   486 non-null    object\n",
      " 2   references  500 non-null    object\n",
      " 3   publishers  37 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 22.0+ KB\n"
     ]
    }
   ],
   "source": [
    "graph_other_data = read_json(\"./data/graph_other_data.json\")\n",
    "graph_other_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>venues_id</th>\n",
       "      <th>references</th>\n",
       "      <th>publishers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>526</td>\n",
       "      <td>486</td>\n",
       "      <td>500</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>492</td>\n",
       "      <td>309</td>\n",
       "      <td>101</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>[{'family': 'Pal', 'given': 'Kamalendu', 'orci...</td>\n",
       "      <td>[issn:2076-3417]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'id': 'crossref:735', 'name': 'Thomas Telford...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>331</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  authors         venues_id  \\\n",
       "count                                                 526               486   \n",
       "unique                                                492               309   \n",
       "top     [{'family': 'Pal', 'given': 'Kamalendu', 'orci...  [issn:2076-3417]   \n",
       "freq                                                    4                15   \n",
       "\n",
       "       references                                         publishers  \n",
       "count         500                                                 37  \n",
       "unique        101                                                 37  \n",
       "top            []  {'id': 'crossref:735', 'name': 'Thomas Telford...  \n",
       "freq          331                                                  1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_other_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the CSVs, also for the data provided inside the JSONs, we found some quantitative difference.\n",
    "\n",
    "Since the final outputs of the software must be Python objects that reflects the data stored in both databases, we will have to check for common entries and diffent ones. We kept this in mind while creating the generic query processor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='software_organization'></a>\n",
    "\n",
    "## 3. Software organization\n",
    "\n",
    "We try to organize the software to be scalable and accessible.\n",
    "It is scalable since we tried to keep the code as general-purpose as possible. The idea is to have a basic structure that can be adapted to different datasets or future implementation of different kind of databases.\n",
    "It is accessible since we designed an **entry point, `main.py`,** foreasly access the software. This feature was also important in the testing and debugging phases.\n",
    "\n",
    "Let's take a look at all the files contained in the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"requirements.txt\"></div>\n",
    "\n",
    "## requirements.txt\n",
    "\n",
    "Python requirements files are a great way to keep track of the Python modules. It is a simple text file that saves a list of the modules and packages required by your project. By creating a Python requirements.txt file, you save yourself the hassle of having to track down and install all of the required modules manually.\n",
    "\n",
    "It makes it easy to share your project with others. They install the same Python modules you have listed in your requirements file and run your project without any problems.\n",
    "\n",
    "In case you ever need to update or add a Python module to your project, you simply update the requirements file rather than having to search through all of your code for every reference to the old module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"const.py\"></div>\n",
    "\n",
    "## const.py\n",
    "\n",
    "In this file we stored all the constants we will need in ou project. This feature allows to easly change the value of elements widly used in the execution. For example you will find the data source local path, the base url of our RDF resources we will create, the path where the relational database file will be stored, and so on.\n",
    "\n",
    "Regarding the triplestore database, we also decide to store here the queries we will use in the `TriplestoreQueryProcessor`, this allows to have a more clear code in the query process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"URIs.py\"></div>\n",
    "\n",
    "## URIs.py\n",
    "\n",
    "The [*Resource Description Framework* (RDF)](https://www.w3.org/RDF/) allows users to describe both Web documents and concepts from the real world—people, organisations, topics, things—in a computer-processable way. Publishing such descriptions on the Web creates the Semantic Web. [URIs (*Uniform Resource Identifiers*)](https://www.w3.org/Addressing/URL/uri-spec.html) are very important, providing both the core of the framework itself and the link between RDF and the Web.\n",
    "\n",
    "To have them all in one place we created a dedicate file. All classes of resources and proprieters that relates them, defined by the UML provided, will be presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"data_model.py\"></div>\n",
    "\n",
    "## data_model.py\n",
    "\n",
    "The structure presented in UML is translated in this file.\n",
    "\n",
    "![datamodel](software-documentation/img/datamodel.png)\n",
    "\n",
    "We define all the Python classes and relative sub-classes.\n",
    "This process is import in order to return Python objects from the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div id=\"common_utils.py\"></div>\n",
    "\n",
    "## common_utils.py\n",
    "\n",
    "Here you can find general function that we used in the programm. They address specific task we need to perform during the execution. You can find in this file custom functions like: `csv_to_df`, `json_to_df` or `blazegraph_instance_is_active` that check trough a HTTP request is the Blazegraph service is active or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"main.py\"></div>\n",
    "\n",
    "## main.py\n",
    "\n",
    "This is the entry point of our program, we launch all the processes we need to obtain our outputs, all wrapped in an `app` function.\n",
    "\n",
    "A key aspect for devolp a complex program like this one was to allow the possibility to run debugs.\n",
    "Since we used Visual Studio Core as editor, we took advantage of one of the key features of this editor, its great debugging support. VS Code's built-in debugger helps accelerate the edit, compile, and debug loop.\n",
    "To be able to do so we setted up the `launch.json` (contained in the `.vscode` folder) file as reported in [this guide by VS Code](https://code.visualstudio.com/docs/editor/debugging).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"data_processors.py\"></div>\n",
    "\n",
    "## data_processors.py\n",
    "\n",
    "In this file we process all the data provided and we build the [pandas DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) we will need for populate our databases.\n",
    "\n",
    "We created a custom class `DataProcessor` that will take as attributes all the DataFrames we will create. As made in other cases, when we need to write or read attributes of a class, we used the respective *set* and *get* methods for each attribute.\n",
    "\n",
    "You can also find the costum `DataProcessor`'s sub-class we created, `GraphDataProcessor`, where we compose all the triplets of our graph database. As properties of this class we setted all the dictonaries we used in the process to create the relation between the different entities we describe. \n",
    "\n",
    "In the original design of the program we had imagined to perform in this file also the creation of the tables for the relational database. In the final design presented here, we instead splited the creation of the tables for the relational database in another file that we will see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"relational_processor.py\"></div>\n",
    "\n",
    "## relational_processor.py #TO CHECK\n",
    "\n",
    "#### Relational Data Processor\n",
    "The upload data function in relational data processors are divided into 2 parts. when we upload csv data we create all tables required for both csv and json. for csv data we populate these: Book, BookChapter, Journal, JournalArticles, PublicationID. Proceeeding and ProceedingPaper are empty according to our data.\n",
    "\n",
    "we also create these empty tables to make it ready to use for json data in next step:\n",
    "Author, Cites, Organization, OrgID, Person, PersonID, and VenueID.\n",
    "\n",
    "with json data, references are linked to publications. Moreover, venue ids is linked to venues. Additionaly, authors and organizations are linked to publications and venues.\n",
    "\n",
    "For creating tables for each publication type we merge citations, authors and venues.\n",
    "\n",
    "In order to link publications to authors, we create author table with dois and personID table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"triplestore_processor.py\"></div>\n",
    "\n",
    "## triplestore_processor.py\n",
    "\n",
    "After the creation of the triplets in the `data_processor.py`, we upload them to our online service, Blazegraph, to be stored and to have a queryable endpoint.\n",
    "\n",
    "The base class `TriplestoreProcessor` sotres in the variable `endpointUrl` the URL of the SPARQL endpoint of the triplestore, initially set as an empty string, that will be updated with the method `setEndpointUrl`\n",
    "\n",
    "The sub-class `TriplestoreDataProcessor`, with its method `uploadData` enables to upload the collection of data specified in the input file path into the database. We check if the file in input is a CSV or JSON and we launch the methods for the creation of the different DataFrames, already created in the `GraphDataProcessor`. Before this step we step if the data are already been uploaded in a precedent execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"query_processors.py\"></div>\n",
    "\n",
    "## query_processors.py\n",
    "\n",
    "Two main classes will be contained in this file: `RelationalQueryProcessor` and `TriplestoreQueryProcessor`. Both classes will have as methods the queries required in the project guidelines. As designed in the \"UML of additional classes\", these two classes will be sub-classes of the respective processor classes (`RelationalProcessor` and `TriplestoreProcessor`) and also both sub-classes of a generic `QueryProcessor`.\n",
    "\n",
    "In this same file we clean the DataFrames returning from each query processor and we combine the two in one DataFrame with the [pandas concat method.](https://pandas.pydata.org/docs/reference/api/pandas.concat.html?highlight=concat#pandas.concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"queries.py\"></div>\n",
    "\n",
    "## query.py\n",
    "\n",
    "We created this costum file to manage the execution of all the queries. From the `main.py` we launche the execution of the function here contained. All the functions here contained will take as input the list of processor we have in our project (this feature allows to add in future also another data processor if needed, like a NoSQL processor for example), after the execution of the generic query, we process the result and translate them into Python object defined in the `data_model.py`. At least we write a *txt* file for each query, these files will be stored in the `queries-results` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"functioning\"></div>\n",
    "\n",
    "# 4.Functioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution of our program starts from the entry point, setted in the `launch.json` file, the `main.py`. The app starts runnining and the first message appears in console to confirm that. \n",
    "\n",
    "The `app` function contains the whole program. As already said, this design, helps create a more clear sequence of steps that the program needs to execute to achive our final goal.\n",
    "\n",
    "Since we will use few external libraries to handle specific tasks, we insert the execution of `app` in a *try/except* statement. We do that to be able to understand if and when an error is produced during the execution, above all if the error occur in a part of the code out of our program, an external library for example. \n",
    "\n",
    "As first thing we see a flag `data_has_been_uploaded` setted by default at *False*. We will need that more further on to check if the data are correctly updated on Blazegraph.\n",
    "\n",
    "Main è l'entry point del nostro programma. `if __name__ == \"__main__\":` fa partire l'esecuzione del programma e lancia il metodo app. La funzione app() contiene all'interno il nostro programma. Tutte l'esecuzione del nostro programma è eseguita qui. Oltre a rispettare le specifiche di un programma Python, aiuta a creare un flusso di esecuzione chiaro. Uqesto facilita la comprensione e il dubbugging. La configurazione del nostro entry point è fatta nel launch.json. \n",
    "\n",
    "La prima cosa che facciamo, dato che utilizziamo funzioni di pandas, libreria esterna, potrebbero dare errore per diversi motivi (parametri sbagliati ecc...). Sempre bene proteggere l'esecuzione del programma quando si utilizzano librerie esterne. Questo permetto di individuare chiaramente dove il programma fallisce.Se anche sola una delle esecuzione comprese nel try fallise, except cattura l'errore e blocca l'esecuzione. Finally è utilizzato per fare quelle azioni necessarie anche se il programma fallisce. Exception è una classe Python che rappresenta l'errore riscontrato, l'oggetto che ci arriva, della classe Exception viene messo in una variabile error che poi stamperemo nel terminale.\n",
    "\n",
    "Le costanti sono state definite tutte con lettere maiuscole come prassi, questo facilita l'individuazione di questi.\n",
    "\n",
    "Il primo flag che incontriamo è data_has_been_uploaded. Questo flag è solo definito e settato a false per adesso. Ci servirà più avanti per controllare il corretto upload dei dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blazegraph is the web service we used for upload or graph database.\n",
    "\n",
    "Blazegraph è un servizio web, abbiamo inserito questa azione (l'esecuzione del servizio) e avviamo in automatico il servizio. L'output della funzione è un boolean. nel caso il servizio non sia attivo, una funzione lo fa partire start_blazegraph_server(). In common utilities raccoglieremo questo tipo di funzioni. Andando dentro vediamo che questa funzione, utilizzando sempre un try, prendiamo il percorso del file nel nostro blazegraph.jar che permetto il collegamento con il servizio web di Blazegraph. La corretta risposta è raccolta in blazegraph_instance_is_active(), utilizzando la libreria requests che permette di fare chiamare HTTP facilmente. Se il codice di risposta è 200 proseguiamo con l'esecuzione del software. \n",
    "\n",
    "Dopo il time.sleep(3) controlliamo nuovamente se blazegraph è attivo. Risettiamo il flag a False per ordine nostro. Generiamo noi l'errore e tramite il raise blocchiamo l'esecuzione del programma. In questo caso il try vai nell'except e poi sul finally per concludere l'esecuzione e riportarci l'errore.\n",
    "\n",
    "Se il servizio è partito, inziamo con i nostri processori.Inziamo dal triplestore. Per evitare di ricaricare le triplette, notando anche la generezione di particolare sovrascrizioni, non ricarichiamo ulteriormente le triplette. In blazegraph_instance_is_empty() utilizziamo una get della libreria di SPARQL. df_sparql prende tre parametri che settiamo e facciamo una query generica di tutto il database. Con questo metodo facciamo una query generica per controllare che ci sia o no qualcosa. Il metodo empty, applicato sul dataframe di risultato, controlla che il dataframe non sia vuoto. Se è vuoto iniziamo a processare le nostre cose, altrimenti scrivo in console che è già stato popolato ed evito di fare tutte quelle azioni necessarie per popolarlo. Nela caso sia vuoto, inizializiamo la classe TriplestoreDataProcessor(). Istanzionado la classe, abbiamo accesso ai suoi metodi. In questo caso .setEndpointUrl viene ereditato dalla super classe TriplestoreProcessor(). Il processor (propietà della classe) ci permetto di elaborare tutti i dataframe necessari. Per fare questo abbiamo creato la classe GraphDataProcessor(), popoleremo la proprietà della ckassa TriplestoreDataProcessor con la classe GraphDataProcessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### main.py\n",
    "\n",
    "In the main file, we launch all the processes we need to obtain our outputs, all wrapped in an `app` function. \n",
    "\n",
    "After checking the correct connection with [Blazegraph](https://blazegraph.com/), an ultra high-performance graph database supporting RDF/SPARQL APIs, where our triplestore database will be stored, we instantiate the `TriplestoreDataProcessor` class. The method `setEndpointUrl` will write the URL as attribute of the `TriplestoreDataProcessor` class.\n",
    "After the triplestore, we set the relational database path where the database will be stored, and instantiate also the `RelationalDataProcessor` class. As for for the triplestore, also for the relational database, the attribute of the class `RelationalProcessor`, `bdPath` will be write by the method `setDbPath`.\n",
    "\n",
    "Both databases need a basic class that sets and gets the relative paths where they are stored. The `RelationalProcessor()` class and the `TriplestoreProcessor()` class will take as attribute this path. Through the set method the path will be written in the class attribute, while with the get method it will be read from the class attribute. \n",
    "\n",
    "In the next step, we create the query processors for both databases, instantiating `TriplestoreQueryProcessor` and `RelationalQueryProcessor`. Both data processor will be appended to a list of processors. This list will be pass as attribute of the `GenericQueryProcessor` class.\n",
    "The variable `queryProcessor` will cointain this list of *QueryProcessor* objects to involve when one of the *get* methods (the actual queries) are executed. In practice, every time a *get* method is executed, the method will call the related method on all the `QueryProcessor` objects included in the variable *queryProcessor*, before combining the results and returning the requested object. \n",
    "\n",
    "After that the query methods are launched, asking about data in both databases. After checking that both query processors are added we run all the generic query functions and we write the records in respective text files.\n",
    "\n",
    "## data_processors.py\n",
    "\n",
    "We read the datasets provided and we build all the DataFrames we need for further operations. The `DataProcessor()` class is the first one of the additional classes we created, even if it wasn't required by the UML diagram provided.\n",
    "\n",
    "We shaped this main class to be able to store the DataFrames we need, passing them as attributes. We then use \"checking\" methods that can handle the set and get all the DataFrames. This design feature is important to break the code into a more understandable series of instructions and it also builds a security level that will check the correctness of the DataFrames that we will build.\n",
    "\n",
    "`DataProcessor()` will be shaped like this:\n",
    "\n",
    "    class DataProcessor(object):\n",
    "        def __init__(self) -> None:\n",
    "            \n",
    "            self.publications_df: DataFrame = DataFrame()\n",
    "            self.authors_df: DataFrame      = DataFrame()\n",
    "            self.references_df: DataFrame   = DataFrame()\n",
    "            self.venues_df: DataFrame       = DataFrame()\n",
    "            self.publishers_df: DataFrame   = DataFrame()\n",
    "\n",
    "Where the attributes will contain the DataFrames we will build later on.\n",
    "\n",
    "In order to be able to write these attributes we will use a set function for all of them. We show here an example of how we handle the Publications DataFrame:\n",
    "\n",
    "    def set_publications_df(self, _publications_df):\n",
    "        # Check if the DataFrame is empty.\n",
    "        if len(_publications_df) < 1 :\n",
    "            print('-- WARN: Publications Data Frame is empty :(')\n",
    "\n",
    "        self.publications_df = _publications_df\n",
    "\n",
    "    def get_publications_df(self) -> DataFrame:\n",
    "        return self.publications_df\n",
    "\n",
    "For each DataFrame a builder function will take as input the original dataset path (CSV or JSON depending on where the data are stored) and will write the relative attribute of the class.\n",
    "Continuing with the Publications DataFrame example, the method will look like this:\n",
    "\n",
    "        def publicationsDfBuilder(self, _csv_f_path: str) -> None:\n",
    "\n",
    "        dtype = {\n",
    "            'id'                 : 'string',\n",
    "            'title'              : 'string',\n",
    "            'type'               : 'string',\n",
    "            'publication-year'   : 'int',\n",
    "            'issue'              : 'string',\n",
    "            'volume'             : 'string',\n",
    "            'chapter'            : 'string',\n",
    "            'pubblication_venue' : 'string',\n",
    "            'venue_type'         : 'string',\n",
    "            'publisher'          : 'string',\n",
    "            'event'              : 'string'\n",
    "        }\n",
    "\n",
    "        publications_df = csv_to_df(_csv_f_path, dtype) \n",
    "        \n",
    "        self.set_publications_df(publications_df)\n",
    "\n",
    "Here we provide a list of all the `DataProcessor()` methods:\n",
    "\n",
    "    set_publications_df(self, _publications_df: DataFrame) -> None:\n",
    "    get_publications_df(self) -> DataFrame:\n",
    "    set_authors_df(self, _publications_df: DataFrame) -> None:\n",
    "    get_authors_df(self) -> DataFrame:\n",
    "    set_publishers_df(self, _publications_df: DataFrame) -> None:\n",
    "    get_publishers_df(self) -> DataFrame:\n",
    "    set_references_df(self, _publications_df: DataFrame) -> None:\n",
    "    get_references_df(self) -> DataFrame:\n",
    "    set_venues_df(self, _publications_df: DataFrame) -> None:\n",
    "    get_venues_df(self) -> DataFrame:\n",
    "\n",
    "    def data_frames_has_been_built(self) -> bool:\n",
    "\n",
    "    def publicationsDfBuilder(self, _csv_f_path: str) -> None:\n",
    "    def referncesDfBuilder(self, _json_f_path: str) -> None:\n",
    "    def venuesDfBuilder(self, _json_f_path: str) -> None:\n",
    "    def authorsDfBuilder(self, _json_f_path: str) -> None:\n",
    "    def publishersDfBuilder(self, _json_f_path: str) -> None:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "TriplestoreDataProcessor() la funzione speciale def __init__(self) sono funzioni interne di Python. L'unica cosa che fa init, nel caso di una sottoclasse è scatanere la superclasse relativa e popola la sua proprietà. sel.processor dichiara la proprietà della classe. Questo processor non è ne una costante, ne una variabile qualunque ma è una proprietà della classe. Creata nel main questa classe delegata a caricare i dati, scateniamo il metodo setEndpointUrl() che ci servirà per andare a caricare i dati nel database passandogli BASE_URL, questo valore, dentro la funzione setEndpointUrl settiamo il parametro _url che useremo, passandogli il dato corretto.\n",
    "\n",
    "L'endpointUrl sarà il base url delle nostre risorse (triplette) che caricheremo sul graph.\n",
    "\n",
    "uploadData ritorna un boolean e ci servirà questo per aggiornare il nostro flag precedente data_has_been_uploaded. Se i files sono validi (files_are_valid) controllo che tipo di file sono, CSV o JSON. Se è un csv il processor precedente processor che abiamo inizializzato, creaimo il data frame delle Publication (metodo della classe GraphDataProcessor) che costruisce il dataframe generico delle Pubblicazione tramite il path che gli abbiamo passato. Invece di scrivere ogni volta le azione che leggono il csv e fanno il corretto data frame necessario, abbiamo racchiuso in una funzione che abbiamo messo nelle nostre common utilities, csv_to_df, che compone il dataframe con i dtype (le colonne che setteremo ad esempio in PublicationDfBuilder) come dictonary.\n",
    "publicationDfBuilder scrive dentro la classe DataProcessor, come parametri, i diversi data frame che creiamo e che ci serviranno per popolare il database.\n",
    "\n",
    "Ogni data frame necessario buildato sarà poi scritto nelle proprietà delle classe. Alla fine controlliamo, con data_frames_has_been_built, se tutti i data frames sono stati fatti correttamente, costriusco le triplette. Lanciamo quindi GraphBuilder per creare tutte le triplette. Nei metodi do_..._triples prendiamo i data frame che ci servono.\n",
    "\n",
    "Triplestore deploy inizializza la connession con .open, gli passiamo l'endpoint due volte perchè la prima sarà per creare la connessione e il secondo sarà per permettere la comunicazione, aggiornarlo -> è scritto cmq nella documentazione della libreria. \n",
    "\n",
    "Per ogni tripletta dentro il graph aggiungi al db online (triplestore.add(triple)). In ongi caso, finito il deploy, chiudo la connessione del triplestore con triplestore.close().\n",
    "\n",
    "Settiamo a questo punto, se non ci sono errori, data_has_been_uploaded a True. Il valore verrà ritornato da uploadData come spechifiche comunicate dal prof. Questo succede nel finally e andrà a ritornare un variabile che appunto sarà data_has_been_uploaded. \n",
    "\n",
    "A questo punto possiamo eseguire le query.\n",
    "\n",
    "Init processors - inzializiamo i processori con i loro metodi. L'istanza di queste due classi, la inseriamo in una lista. instaziamo il generic query processor e gli mettiamo dentro la lista con un metodo della classe GenericQueryProcessor (addQueryProcessor) e ti restituisce True o Flase. \n",
    "Se questo è fatto correttamente il GenericQuery avrà dentro tutti e due i processors. Nel caso avessi più di due tipi di database, la scatola GenericQueryProcessor potrebbe contenerlo senza problemi. Questo è stato fatto perchè il metodo addQuery scrive dentro la propria proprietà (la lista) i diversi processor. Il Generic ha come proprietà la lista dei diversi processor query che abbiamo aggiunto. Generic ha dentro tutti i processor. get.publicationByauthorId è dentro il generic non dentro i diversi processor, perchè le query sono le stesse anche se scritte in linguaggi diversi (SQL e SPARQL). La chimata di ogni query, a secondo del tipo di processor, è eseguita una volata sola ma fa cose diverse a seconda del tipo di Database. \n",
    "\n",
    "I due metodi relativi a relational e graph restituiscono i data frame di risultato. Una volta uniti i due dataframe di risultato dei due database, viene fatta una lista e restituiamo così un Python object.\n",
    "La stessa lociga è applicata a tutte le query. \n",
    "\n",
    "I metodi set e get permettono di tenere al \"sicuro\" le proprietà. Se deve essere una scritta il controllo sarà effettuato nel set ad esempio. Questo aggiunge un livello di sicurezza ed evita di scrivere, ad esempio, un numero invece che una stringa. Abbiamo sfruttato l'occasiane di usare questi metodi dlegati per proteggere le nostre proprietà delle classi. In graphdatsprocessor in setGraph ad esempio è impostato già un controllo che vede che il graph non sia vuoto.\n",
    "\n",
    "TriplestoreDataProcessor contiene una propprietà che è il vero processore dei dati che è frutto di un'ulteriore classe, GraphDataProcessor. Così giustifichiamo anche la creazione della classe TriplestoreDataProcessor che effettivamente processa i dati prima di caricarli.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
